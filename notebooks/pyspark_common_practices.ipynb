{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom schema in pyspark\n",
    "# Importing files\n",
    "# various analytics functionalities ( selectiong columns, filtering on the basis of conditions,aggregation functions)\n",
    "# Sql Querries\n",
    "# Joins \n",
    "# Missing data handling \n",
    "# optimization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/24 11:13:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/07/24 11:13:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"pyspark_practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom schema in pyspark\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "data = [\n",
    "    (1,\"john\",\"delhi\"),\n",
    "    (2,\"stark\",\"new yark\"),\n",
    "    (3,\"ronita\",\"paris\")\n",
    "]\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"emp_name\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+\n",
      "| id|emp_name|    city|\n",
      "+---+--------+--------+\n",
      "|  1|    john|   delhi|\n",
      "|  2|   stark|new yark|\n",
      "|  3|  ronita|   paris|\n",
      "+---+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_df = spark.createDataFrame(data,schema=schema)\n",
    "\n",
    "custom_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing files data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+\n",
      "|dept_id|dept_name|emp_id|\n",
      "+-------+---------+------+\n",
      "|      1|       HR|     1|\n",
      "|      2|  Finance|     2|\n",
      "|      3|       IT|     3|\n",
      "|      4|Marketing|     4|\n",
      "|      5|    Sales|     5|\n",
      "|      6|       HR|     6|\n",
      "|      7|  Finance|     7|\n",
      "|      8|       IT|     8|\n",
      "|      9|Marketing|     9|\n",
      "|     10|    Sales|    10|\n",
      "|     11|       HR|    11|\n",
      "|     12|  Finance|    12|\n",
      "|     13|       IT|    13|\n",
      "|     14|Marketing|    14|\n",
      "|     15|    Sales|    15|\n",
      "|     16|       HR|    16|\n",
      "|     17|  Finance|    17|\n",
      "|     18|       IT|    18|\n",
      "|     19|Marketing|    19|\n",
      "|     20|    Sales|    20|\n",
      "+-------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"../data/department_pipe_delimeted.csv\"\n",
    "import_df = spark.read.csv(file_path,inferSchema=True,header=True,sep=\"|\")\n",
    "import_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+\n",
      "|dept_id|dept_name|emp_id|\n",
      "+-------+---------+------+\n",
      "|      1|       HR|     1|\n",
      "|      2|  Finance|     2|\n",
      "|      3|       IT|     3|\n",
      "|      4|Marketing|     4|\n",
      "|      5|    Sales|     5|\n",
      "|      6|       HR|     6|\n",
      "|      7|  Finance|     7|\n",
      "|      8|       IT|     8|\n",
      "|      9|Marketing|     9|\n",
      "|     10|    Sales|    10|\n",
      "|     11|       HR|    11|\n",
      "|     12|  Finance|    12|\n",
      "|     13|       IT|    13|\n",
      "|     14|Marketing|    14|\n",
      "|     15|    Sales|    15|\n",
      "|     16|       HR|    16|\n",
      "|     17|  Finance|    17|\n",
      "|     18|       IT|    18|\n",
      "|     19|Marketing|    19|\n",
      "|     20|    Sales|    20|\n",
      "+-------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"../data/department_pipe_delimeted.csv\"\n",
    "custom_import_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").option(\"delimiter\",\"|\").load(file_path)\n",
    "custom_import_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics with Pyspak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|dept_name|emp_id|\n",
      "+---------+------+\n",
      "|       HR|     1|\n",
      "|  Finance|     2|\n",
      "|       IT|     3|\n",
      "|Marketing|     4|\n",
      "|    Sales|     5|\n",
      "|       HR|     6|\n",
      "|  Finance|     7|\n",
      "|       IT|     8|\n",
      "|Marketing|     9|\n",
      "|    Sales|    10|\n",
      "|       HR|    11|\n",
      "|  Finance|    12|\n",
      "|       IT|    13|\n",
      "|Marketing|    14|\n",
      "|    Sales|    15|\n",
      "|       HR|    16|\n",
      "|  Finance|    17|\n",
      "|       IT|    18|\n",
      "|Marketing|    19|\n",
      "|    Sales|    20|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select the particular columns\n",
    "file_path = r\"../data/department_pipe_delimeted.csv\"\n",
    "\n",
    "analytics_df = spark.read.csv(file_path,inferSchema=True,header=True,sep=\"|\")\n",
    "\n",
    "analytics_df.select(\"dept_name\",\"emp_id\").show()\n",
    "\n",
    "\n",
    "# analytics_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+------+\n",
      "|emp_id|       emp_name|salary|\n",
      "+------+---------------+------+\n",
      "|     2|     Jane Smith|  7000|\n",
      "|     3|Michael Johnson|  8000|\n",
      "|     5|  William Brown|  9000|\n",
      "|     7|  Richard Moore|  7500|\n",
      "|     8| Jessica Taylor|  8500|\n",
      "|    10|Patricia Thomas|  7200|\n",
      "|    12|    Susan White|  8100|\n",
      "|    13|  Daniel Harris|  6600|\n",
      "|    14|   Sarah Martin|  7700|\n",
      "|    15|  Paul Thompson|  8800|\n",
      "|    16|   Karen Garcia|  6900|\n",
      "|    19|   George Clark|  7000|\n",
      "+------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filtering data on some  conditions\n",
    "file_path = r\"../data/employee_pipe_delimeted.csv\"\n",
    "emp_df = spark.read.csv(file_path,header=True,inferSchema=True,sep=\"|\")\n",
    "\n",
    "emp_df.filter(\"salary >6500\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------+-------+---------+------+\n",
      "|emp_id|           emp_name|salary|dept_id|dept_name|emp_id|\n",
      "+------+-------------------+------+-------+---------+------+\n",
      "|     1|           John Doe|  5000|      1|       HR|     1|\n",
      "|     2|         Jane Smith|  7000|      2|  Finance|     2|\n",
      "|     3|    Michael Johnson|  8000|      3|       IT|     3|\n",
      "|     4|        Emily Davis|  6000|      4|Marketing|     4|\n",
      "|     5|      William Brown|  9000|      5|    Sales|     5|\n",
      "|     6|       Linda Wilson|  5500|      6|       HR|     6|\n",
      "|     7|      Richard Moore|  7500|      7|  Finance|     7|\n",
      "|     8|     Jessica Taylor|  8500|      8|       IT|     8|\n",
      "|     9|   Charles Anderson|  6500|      9|Marketing|     9|\n",
      "|    10|    Patricia Thomas|  7200|     10|    Sales|    10|\n",
      "|    11|Christopher Jackson|  5400|     11|       HR|    11|\n",
      "|    12|        Susan White|  8100|     12|  Finance|    12|\n",
      "|    13|      Daniel Harris|  6600|     13|       IT|    13|\n",
      "|    14|       Sarah Martin|  7700|     14|Marketing|    14|\n",
      "|    15|      Paul Thompson|  8800|     15|    Sales|    15|\n",
      "|    16|       Karen Garcia|  6900|     16|       HR|    16|\n",
      "|    17|      Mark Martinez|  5800|     17|  Finance|    17|\n",
      "|    18|     Nancy Robinson|  6200|     18|       IT|    18|\n",
      "|    19|       George Clark|  7000|     19|Marketing|    19|\n",
      "|    20|   Sandra Rodriguez|  6300|     20|    Sales|    20|\n",
      "+------+-------------------+------+-------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Union methods\n",
    "file_path =r\"../data/department_pipe_delimeted.csv\"\n",
    "dept_df = spark.read.csv(file_path, inferSchema=True, header=True, sep=\"|\")\n",
    "\n",
    "combined_df = emp_df.join(dept_df,emp_df.emp_id ==dept_df.emp_id)\n",
    "\n",
    "combined_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|     6950.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df.agg({\"salary\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|dept_name|max(salary)|\n",
      "+---------+-----------+\n",
      "|    Sales|       9000|\n",
      "|       HR|       6900|\n",
      "|  Finance|       8100|\n",
      "|Marketing|       7700|\n",
      "|       IT|       8500|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df.groupBy(\"dept_name\").agg({\"salary\":\"max\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|dept_name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|    Sales|      31300|\n",
      "|       HR|      22800|\n",
      "|  Finance|      28400|\n",
      "|Marketing|      27200|\n",
      "|       IT|      29300|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df.groupBy(\"dept_name\").agg({\"salary\":\"sum\"}).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL queries with pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Query\n",
    "\n",
    "combined_df.createOrReplaceTempView(\"combined_table\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+------+-------+---------+------+\n",
      "|emp_id|      emp_name|salary|dept_id|dept_name|emp_id|\n",
      "+------+--------------+------+-------+---------+------+\n",
      "|     5| William Brown|  9000|      5|    Sales|     5|\n",
      "|     8|Jessica Taylor|  8500|      8|       IT|     8|\n",
      "|    12|   Susan White|  8100|     12|  Finance|    12|\n",
      "|    15| Paul Thompson|  8800|     15|    Sales|    15|\n",
      "+------+--------------+------+-------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"select * from combined_table\"\n",
    "\n",
    "query2 = \"select * from combined_table where salary >8000\"\n",
    "\n",
    "\n",
    "spark.sql(query2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n",
      "| id| name| age|\n",
      "+---+-----+----+\n",
      "|  1|Alice|  25|\n",
      "|  2|  Bob|null|\n",
      "|  3| null|  28|\n",
      "|  4|David|  32|\n",
      "|  5|Emily|null|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, mean\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", 25),\n",
    "    (2, \"Bob\", None),\n",
    "    (3, None, 28),\n",
    "    (4, \"David\", 32),\n",
    "    (5, \"Emily\", None)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "temp_df = spark.createDataFrame(data, schema) \n",
    "temp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in column 'age': 2\n"
     ]
    }
   ],
   "source": [
    "# find out the missing data\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "\n",
    "column= 'age'\n",
    "missing_count = temp_df.filter(col(column).isNull() | isnan(col(column))).count()\n",
    "print(f\"Missing values in column '{column}': {missing_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in column 'id': 0\n",
      "Missing values in column 'name': 1\n",
      "Missing values in column 'age': 2\n"
     ]
    }
   ],
   "source": [
    "for column in temp_df.columns:\n",
    "    missing_count = temp_df.filter(col(column).isNull() | isnan(col(column))).count()\n",
    "    print(f\"Missing values in column '{column}': {missing_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 25|\n",
      "|  4|David| 32|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Alice|\n",
      "|  Bob|\n",
      "|David|\n",
      "|Emily|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df.select(\"name\").dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n",
      "| id| name| age|\n",
      "+---+-----+----+\n",
      "|  1|Alice|  25|\n",
      "|  2|  Bob|null|\n",
      "|  4|David|  32|\n",
      "|  5|Emily|null|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df.dropna(subset=[\"name\"]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill the missing data with values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 25|\n",
      "|  2|  Bob|  0|\n",
      "|  3| null| 28|\n",
      "|  4|David| 32|\n",
      "|  5|Emily|  0|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_replaced = temp_df.withColumn(\"age\", when(col(\"age\").isNull(), 0).otherwise(col(\"age\")))\n",
    "df_replaced.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.0\n",
      "+---+-----+----+\n",
      "| id| name| age|\n",
      "+---+-----+----+\n",
      "|  1|Alice|25.0|\n",
      "|  2|  Bob|28.0|\n",
      "|  3| null|28.0|\n",
      "|  4|David|32.0|\n",
      "|  5|Emily|28.0|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "mean_age = temp_df.select(round(mean(col(\"age\"))).alias(\"mean_age\")).collect()[0][0]\n",
    "print(mean_age)\n",
    "df_replaced = temp_df.withColumn(\"age\", when(col(\"age\").isNull(), mean_age).otherwise(col(\"age\")))\n",
    "df_replaced.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
